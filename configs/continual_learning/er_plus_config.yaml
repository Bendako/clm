# ER+ (Experience Replay with Regularization) Configuration

# Model configuration
model:
  type: "mlp"  # Model type: mlp, cnn, lstm, etc.
  hidden_sizes: [256, 128]  # Hidden layer sizes for MLP
  dropout: 0.2  # Dropout rate

# Training configuration
epochs: 5
batch_size: 64
optimizer:
  type: "adam"
  lr: 0.001
  weight_decay: 0.0001

# Continual learning strategy configuration
continual_strategy:
  # ER+ strategy
  er_plus:
    enabled: true
    memory_size: 500  # Replay buffer size
    batch_size: 32    # Batch size for sampling from replay buffer
    reg_weight: 1.0   # Weight for regularization loss
    temperature: 2.0  # Temperature for softening logits
    task_balanced: true  # Whether to maintain balanced samples per task

  # Disable other strategies
  ewc:
    enabled: false
  lwf:
    enabled: false
  gem:
    enabled: false
  pnn:
    enabled: false
  packnet:
    enabled: false

  # Replay buffer (not needed as ER+ has its own buffer)
  replay:
    enabled: false

# Evaluation configuration
evaluation:
  metrics: ["accuracy", "forgetting"]
  task_eval_frequency: 1  # Evaluate after every task
  save_model: true

# Logging configuration
logging:
  level: "info"
  log_gradients: false
  log_parameters: false

# Experiment tracking
experiment:
  name: "er_plus_experiment"
  tags: ["er_plus", "continual_learning"]
  notes: "Testing ER+ strategy for continual learning" 